{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data used in Convotate\n",
    "This notebook outlines steps in data cleaning in the development of Convotate. Our initial sequence dump was taken from 23k taxonomically dereplicated/diverse genomes based on the taxIDs given by GTDB. Basic initial filtering, which consisted of removal of duplicate sequences and removal of sequences <30AA long, was done in bash. This provides example code/data for how we dealt with very rare subsystems, and the files/code used to merge the ontology. We have not provided the full sequence dumps as they are over 30GB in compressed format, however, you may access/see:\n",
    "1. genome ID dump from GTDB release 83 (.tree), which lists NCBI tax IDs: https://data.ace.uq.edu.au/public/gtdb/data/releases/release83/83.0/\n",
    "2. all raw data from PATRIC (www.patricbrc.org), ftp: ftp://ftp.patricbrc.org\n",
    "3. our final cleaned data (~3.34GB compressed tar.bz2 file), consisting of ~14 million training, ~4 million test sequences, and ontology/label files, here: https://www.dropbox.com/s/baz9dq3obhcz5lq/FinalDataFiles.tar.bz2?dl=0\n",
    "\n",
    "Required packages:\n",
    "pandas, numpy, itertools, python-levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "import Levenshtein\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Match for diverse random sequence selection\n",
    "About 200 subsystems did not have enough data (1000 sequences or more). We searched for sequences assigned to those subsystems in the full PATRIC sequence dump, removed duplicates, and then used string fuzzy match to get a 'diverse' random selection of these as demonstrated below. If there were fewer than 1000 sequences in a subsystem for the complete PATRIC back end, we excluded it from the study. This is one (very small) example with the subsystem 'Cytolethal Distending Toxins' - only 6000 sequences in the dump. The data file contains columns of: PATRIC tax id (maps to NCBI id), protein ID, ontology columns, AA sequences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/fuzzy_match/Cytolethal distending toxins is computing\n",
      "calculating Levenshtein distances\n",
      "the bins are [0.15181518151815182, 0.19422442244224422, 0.23663366336633662, 0.27904290429042905, 0.3214521452145215, 0.36386138613861385, 0.4062706270627063, 0.44867986798679865, 0.4910891089108911, 0.5334983498349835, 0.5759075907590758, 0.6183168316831683, 0.6607260726072608, 0.7031353135313532, 0.7455445544554455, 0.7879537953795379, 0.8303630363036303, 0.8727722772277227, 0.9151815181518153, 0.9575907590759076, 1.0]\n",
      "The bin counts are \n",
      "     binned\n",
      "10    2558\n",
      "6     1822\n",
      "11     638\n",
      "5      301\n",
      "3      203\n",
      "8      126\n",
      "4      119\n",
      "20      95\n",
      "9       80\n",
      "12      71\n",
      "2       46\n",
      "7       37\n",
      "19      13\n",
      "1        3\n",
      "14       2\n",
      "13       1\n",
      "15       1\n",
      "16       1\n",
      "17       0\n",
      "18       0\n",
      "writing 5069 selected sequences to file for ./data/fuzzy_match/Cytolethal distending toxins\n"
     ]
    }
   ],
   "source": [
    "for filepath in glob.iglob('./data/fuzzy_match/*'):\n",
    "    print(filepath,'is computing')\n",
    "    \n",
    "    # load sequences\n",
    "    sequences = pd.read_csv(filepath, sep='\\t',header=None,names=['feature.patric_id', 'feature.aa_sequence'],usecols=[1,6])\n",
    "    aa_string1=sequences.iloc[0][1]\n",
    "    figID1=sequences.iloc[0][0]\n",
    "    \n",
    "    ## calc Levenshtein distance and append to the dataframe\n",
    "    print('calculating Levenshtein distances')\n",
    "    distance = []\n",
    "    for index, row in sequences.iterrows():\n",
    "        distance.append(Levenshtein.ratio(aa_string1, row[\"feature.aa_sequence\"]))\n",
    "    \n",
    "    sequences['ldistance']=distance\n",
    "    \n",
    "    ## set up sampling - levenshtein bins \n",
    "    # create uniform bin size (20 bins in total)\n",
    "    bins=np.linspace(min(sequences['ldistance']),max(sequences['ldistance']),21).tolist()\n",
    "    labels = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "    sequences['binned'] = pd.cut(sequences['ldistance'], bins=bins, labels=labels)\n",
    "    print('the bins are',bins)\n",
    "    \n",
    "    ## figure out how many samples to take per bin, create sampling function\n",
    "\n",
    "    nbin_counts=sequences['binned'].value_counts().to_frame()\n",
    "    nbins_large=nbin_counts[nbin_counts> 500].count()\n",
    "    nsamp=int(5000/(nbins_large))\n",
    "    print('The bin counts are','\\n',nbin_counts)\n",
    "    \n",
    "    def balanced_randsample(s):\n",
    "        try:\n",
    "            return s.sample(min(len(s), nsamp))\n",
    "        except: pass\n",
    "    \n",
    "    ## take sample\n",
    "    balanced_sequence_set=sequences.groupby('binned')['feature.patric_id'].apply(lambda s: balanced_randsample(s))\n",
    "    balanced_sequence_set=balanced_sequence_set.to_frame().reset_index(level=0, drop=True).reset_index()\n",
    "    \n",
    "    ## look up patric ID indices in aa sequence frame, return list to write to file\n",
    "    selected_sequences=[]\n",
    "    for row in balanced_sequence_set['index']:\n",
    "        get_elements=sequences.iloc[row][0:2]\n",
    "        elements=get_elements['feature.patric_id'],get_elements['feature.aa_sequence']\n",
    "        selected_sequences.append(elements)\n",
    "    \n",
    "    ## write sample to file\n",
    "    print('writing',len(selected_sequences),'selected sequences to file for',filepath)\n",
    "\n",
    "    # with open('./data/fuzzy_match/new_sequences.csv','w') as f:\n",
    "    #     writer = csv.writer(f)\n",
    "    #     writer.writerow(['feature.patric_id', 'feature.aa_sequence'])\n",
    "    #     writer.writerows(selected_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Subsystems and Ontology\n",
    "As the SEED is a manually curated system, some Subsystems have significant overlap - or are even subsets of another. Subsystems are created with groups of 'roles' (EC assignments or the like). We assessed roles per subystem and merged if there was >=80% overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the files and do some preprocessing\n",
    "Import the information about which roles are in which subsystems (manually taken from the PATRIC website due to API issues). Convert roles IDs to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subsystem Name</th>\n",
       "      <th>Roles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D-Galacturonate and D-Glucuronate Utilization</td>\n",
       "      <td>• Mannonate dehydratase (EC 4.2.1.8) • D-manno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flagellum</td>\n",
       "      <td>• Flagellar motor rotation protein MotB • Flag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Type 4 secretion and conjugative transfer</td>\n",
       "      <td>• Inner membrane protein of type IV secretion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ribosomal proteins, single-copy</td>\n",
       "      <td>• LSU ribosomal protein L11p (L12e) • SSU ribo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Folate Biosynthesis</td>\n",
       "      <td>• Serine hydroxymethyltransferase (EC 2.1.2.1)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Subsystem Name  \\\n",
       "0  D-Galacturonate and D-Glucuronate Utilization   \n",
       "1                                      Flagellum   \n",
       "2      Type 4 secretion and conjugative transfer   \n",
       "3                Ribosomal proteins, single-copy   \n",
       "4                            Folate Biosynthesis   \n",
       "\n",
       "                                               Roles  \n",
       "0  • Mannonate dehydratase (EC 4.2.1.8) • D-manno...  \n",
       "1  • Flagellar motor rotation protein MotB • Flag...  \n",
       "2  • Inner membrane protein of type IV secretion ...  \n",
       "3  • LSU ribosomal protein L11p (L12e) • SSU ribo...  \n",
       "4  • Serine hydroxymethyltransferase (EC 2.1.2.1)...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first role uid is: 0\n"
     ]
    }
   ],
   "source": [
    "# Import subsystems and complete list of roles\n",
    "ssRoleNameDf=pd.read_csv('./data/ontology_merging/ss_rolenames_import', sep='\\t', \n",
    "                         header=None).rename(columns={0:'Subsystem Name',1:'Roles'})\n",
    "roleUnique=pd.read_csv('./data/ontology_merging/all_roles', sep='\\t', \n",
    "                       header=None).rename(columns={0:'roleName'})\n",
    "\n",
    "# Create dictionary linking all roles to an integer uid\n",
    "RoleDictNames = {}\n",
    "RoleDictNames=dict(sorted(ssRoleNameDf.values.tolist()))\n",
    "\n",
    "RoleDict = {}\n",
    "for i in range(len(roleUnique)):\n",
    "    RoleDict[roleUnique.loc[i,:].values[0]] = i\n",
    "\n",
    "    \n",
    "# Print the dataframe and the first role\n",
    "display(ssRoleNameDf.head())     \n",
    "\n",
    "print('The first role uid is:',RoleDict.get('(2E,6E)-farnesyl diphosphate synthase (EC 2.5.1.10)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the subsystem-role dataframe to a dictionary with subsystem names as a key\n",
    "## and roles as values (in a list). Clean up the bullets, whitespaces etc and then convert to uids.\n",
    "\n",
    "# Create the dictionary\n",
    "subsystemRoleName=dict(sorted(ssRoleNameDf.values.tolist()))\n",
    "\n",
    "# Clean up the formatting and then convert to uids\n",
    "for key in subsystemRoleName:\n",
    "    s1 = subsystemRoleName.get(key)\n",
    "    units = s1.split('•')\n",
    "\n",
    "    for i in range(len(units)):\n",
    "        units[i] = units[i].strip()\n",
    "\n",
    "    while('' in units) : \n",
    "        units.remove('')\n",
    "\n",
    "    for i in range(len(units)):\n",
    "        units[i]=RoleDict.get(units[i])    \n",
    "    units=sorted(units)\n",
    "    subsystemRoleName.update({key: units})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the similar subsystems\n",
    "Merge subsystems if there is >=80% similarity between their roles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 3 subsystems which need to be merged are: \n",
      " [['2-oxoglutarate dehydrogenase', 'Lipoylated proteins'], ['2-oxoglutarate dehydrogenase', 'TCA Cycle'], ['ATP-dependent Nuclease', 'DNA repair, bacterial RecBCD pathway']]\n"
     ]
    }
   ],
   "source": [
    "## Create a list of classes which need to be merged together, based on their role overlaps\n",
    "\n",
    "mergeClasses=[]\n",
    "\n",
    "for key1, key2 in combinations(subsystemRoleName.keys(), r = 2):\n",
    "    subsystem1_rolelist = subsystemRoleName.get(key1)\n",
    "    subsystem2_rolelist = subsystemRoleName.get(key2)\n",
    "    is_subset = []\n",
    "   \n",
    "    if len(subsystem1_rolelist) <= len(subsystem2_rolelist):\n",
    "        if len(subsystem1_rolelist) > 1:\n",
    "            is_subset=list(map(lambda each: each in subsystem2_rolelist, subsystem1_rolelist))\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        if len(subsystem2_rolelist) > 1:\n",
    "            is_subset=list(map(lambda each: each in subsystem1_rolelist, subsystem2_rolelist))\n",
    "        else:\n",
    "            continue \n",
    "    intersec = np.int0(is_subset)\n",
    "    \n",
    "    if sum(intersec)/len(intersec) > 0.79:\n",
    "        mergeClasses.append([key1,key2])\n",
    "        \n",
    "print('The first 3 subsystems which need to be merged are: \\n',mergeClasses[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list merge function from stackoverflow\n",
    "def merge(lists, results=None):\n",
    "    if results is None:\n",
    "        results = []\n",
    "\n",
    "    if not lists:\n",
    "        return results\n",
    "\n",
    "    first = lists[0]\n",
    "    merged = []\n",
    "    output = []\n",
    "\n",
    "    for li in lists[1:]:\n",
    "        for i in first:\n",
    "            if i in li:\n",
    "                merged = merged + li\n",
    "                break\n",
    "        else:\n",
    "            output.append(li)\n",
    "\n",
    "    merged = merged + first\n",
    "    results.append(list(set(merged)))\n",
    "\n",
    "    return merge(output, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some cases required >2 subsystems to be merged together (i.e. there are three or more  overlaps). we then recursively merge the pairs listed in the output (any common elements mean they end up in the same bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullyMergedClasses=merge(mergeClasses)\n",
    "fullyMergedClasses=merge(fullyMergedClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were then appended to the appropriate row as a new column 'Subsystem Merged' in the ontology. We used a delimiter of '!' between concatenated subsystems to prevent confusion arising from the fact commas were in some labels, and keeping substring quotes was causing issues with automatic text parsing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}